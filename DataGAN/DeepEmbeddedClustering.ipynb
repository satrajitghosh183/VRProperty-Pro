{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot load file containing pickled data when allow_pickle=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Samsung\\DeepEmbeddedClustering.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Samsung/DeepEmbeddedClustering.ipynb#W1sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m autoencoder, encoder\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Samsung/DeepEmbeddedClustering.ipynb#W1sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# Load the dataset\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Samsung/DeepEmbeddedClustering.ipynb#W1sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Replace 'path_to_dataset' with the actual path to your dataset file\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Samsung/DeepEmbeddedClustering.ipynb#W1sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mD:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mSamsung\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39marchive\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mdiabetes.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Samsung/DeepEmbeddedClustering.ipynb#W1sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# Normalize the data\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Samsung/DeepEmbeddedClustering.ipynb#W1sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m/\u001b[39m \u001b[39m255\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\satra\\.conda\\envs\\Tensorflow-License-Plate-Detection\\lib\\site-packages\\numpy\\lib\\npyio.py:418\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    416\u001b[0m     \u001b[39m# Try a pickle\u001b[39;00m\n\u001b[0;32m    417\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_pickle:\n\u001b[1;32m--> 418\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot load file containing pickled data \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    419\u001b[0m                          \u001b[39m\"\u001b[39m\u001b[39mwhen allow_pickle=False\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    420\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    421\u001b[0m         \u001b[39mreturn\u001b[39;00m pickle\u001b[39m.\u001b[39mload(fid, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_kwargs)\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot load file containing pickled data when allow_pickle=False"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Define the deep embedding model\n",
    "def deep_embedding_model(input_shape, encoding_dim):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "    decoded = Dense(input_shape[0], activation='sigmoid')(encoded)\n",
    "\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    encoder = Model(input_layer, encoded)\n",
    "\n",
    "    return autoencoder, encoder\n",
    "\n",
    "# Load the dataset\n",
    "# Replace 'path_to_dataset' with the actual path to your dataset file\n",
    "data = np.load(r'D:\\Samsung\\archive\\diabetes.csv')\n",
    "\n",
    "# Normalize the data\n",
    "data = data.astype('float32') / 255\n",
    "\n",
    "# Define the hyperparameters\n",
    "input_shape = data.shape[1:]\n",
    "encoding_dim = 64\n",
    "batch_size = 256\n",
    "epochs = 50\n",
    "\n",
    "# Create the deep embedding model\n",
    "autoencoder, encoder = deep_embedding_model(input_shape, encoding_dim)\n",
    "\n",
    "# Compile the model\n",
    "autoencoder.compile(optimizer=Adam(), loss='mse')\n",
    "\n",
    "# Train the deep embedding model\n",
    "autoencoder.fit(data, data,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                shuffle=True)\n",
    "\n",
    "# Obtain the deep embeddings for the data\n",
    "embeddings = encoder.predict(data)\n",
    "\n",
    "# Perform clustering using K-means\n",
    "kmeans = KMeans(n_clusters=10)\n",
    "cluster_labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# Print the cluster labels\n",
    "print(cluster_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1787\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1740\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1695\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1650\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1606\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1563\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1521\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1478\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1436\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1394\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1352\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1311\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1269\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1226\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1185\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1142\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1100\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.1058\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.1017\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0975\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0935\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0896\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0857\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 0s 1000us/step - loss: 0.0819\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0783\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0749\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0716\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0685\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0656\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0628\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0602\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0579\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0556\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0535\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0516\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0497\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0480\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0464\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0449\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0435\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0421\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0408\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0396\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0385\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0373\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0363\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0353\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0343\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0334\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0325\n",
      "24/24 [==============================] - 0s 739us/step\n",
      "[1 0 1 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0\n",
      " 1 1 1 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0\n",
      " 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1\n",
      " 1 0 0 1 1 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 1 1 1 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 0 1 0 1 0 0 0 0 0\n",
      " 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1\n",
      " 0 0 0 0 0 1 1 0 1 1 0 0 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 0 0 0 0 0 1 1 0 0 1\n",
      " 1 0 1 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 1 1 0 0 0 1 1 1 0 0\n",
      " 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1\n",
      " 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 0 1 0 0 1\n",
      " 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 1 0 1 0 0 1 0 1 1 0 1 0 1 0 1\n",
      " 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1\n",
      " 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1\n",
      " 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0\n",
      " 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0\n",
      " 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1\n",
      " 0 0 0 1 0 1 1 1 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1\n",
      " 1 0 0 1 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\satra\\.conda\\envs\\Tensorflow-License-Plate-Detection\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define the deep embedding model\n",
    "def deep_embedding_model(input_shape, encoding_dim):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "    decoded = Dense(input_shape[0], activation='sigmoid')(encoded)\n",
    "\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    encoder = Model(input_layer, encoded)\n",
    "\n",
    "    return autoencoder, encoder\n",
    "\n",
    "# Load the dataset using pandas\n",
    "# Replace 'path_to_dataset' with the actual path to your dataset file\n",
    "data = pd.read_csv(r'D:\\Samsung\\archive\\diabetes.csv')\n",
    "\n",
    "# Extract the features (columns) from the dataset\n",
    "features = data.columns[:-1]\n",
    "\n",
    "# Normalize the data\n",
    "data[features] = data[features].astype('float32') / 255\n",
    "\n",
    "# Convert the data to a numpy array\n",
    "data = data.to_numpy()\n",
    "\n",
    "# Define the hyperparameters\n",
    "input_shape = data.shape[1:]\n",
    "encoding_dim = 64\n",
    "batch_size = 256\n",
    "epochs = 50\n",
    "\n",
    "# Create the deep embedding model\n",
    "autoencoder, encoder = deep_embedding_model(input_shape, encoding_dim)\n",
    "\n",
    "# Compile the model\n",
    "autoencoder.compile(optimizer=Adam(), loss='mse')\n",
    "\n",
    "# Train the deep embedding model\n",
    "autoencoder.fit(data, data,\n",
    "                batch_size=batch_size,\n",
    "                epochs=epochs,\n",
    "                shuffle=True)\n",
    "\n",
    "# Obtain the deep embeddings for the data\n",
    "embeddings = encoder.predict(data)\n",
    "\n",
    "# Perform clustering using K-means\n",
    "kmeans = KMeans(n_clusters=2)  # Assuming 'Outcome' has 2 classes\n",
    "cluster_labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# Print the cluster labels\n",
    "print(cluster_labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-License-Plate-Detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
