{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Read individual CSV files and store them in a list\n",
    "file_paths = glob.glob(r\"D:\\Housing Prices Prediction\\All Metropolitan Cities\\*.csv\")\n",
    "dfs = []\n",
    "city_names = []\n",
    "for file_path in file_paths:\n",
    "    city_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    df = pd.read_csv(file_path)\n",
    "    dfs.append(df)\n",
    "    city_names.extend([city_name] * len(df))\n",
    "\n",
    "# Combine all dataframes and city names into one dataframe\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "combined_df[\"City\"] = city_names\n",
    "\n",
    "# Select 16,000 random data points\n",
    "random_df = combined_df.sample(n=16000, random_state=42)\n",
    "\n",
    "# Reset index to add a unique index to each row\n",
    "random_df = random_df.reset_index(drop=True)\n",
    "\n",
    "# Save the randomly selected dataframe to a new CSV file\n",
    "random_df.to_csv(\"random_housing_dataset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "csv_file_path = r'D:\\Housing Prices Prediction\\random_housing_dataset.csv'\n",
    "text_file_path = r'D:\\Housing Prices Prediction\\output.txt'\n",
    "\n",
    "with open(csv_file_path, 'r') as csv_file, open(text_file_path, 'w') as text_file:\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        for row in csv_reader:\n",
    "            row_str = ', '.join(str(item) for item in row)\n",
    "            text_file.write(f'({row_str}),\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a DataFrame\n",
    "df = pd.read_csv('random_housing_dataset.csv')\n",
    "\n",
    "# Generate unique numeric IDs\n",
    "ids = range(1, len(df) + 1)\n",
    "\n",
    "# Add the IDs as a new column\n",
    "df['ID'] = ids\n",
    "\n",
    "# Verify that the IDs are unique\n",
    "if len(df['ID'].unique()) == len(df):\n",
    "    print(\"IDs are unique.\")\n",
    "else:\n",
    "    print(\"IDs are not unique.\")\n",
    "\n",
    "# Save the updated dataset to a new file\n",
    "df.to_csv('updated_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import pickle\n",
    "from flask import Flask, jsonify, request\n",
    "\n",
    "\n",
    "# Read the dataset\n",
    "df = pd.read_csv(\"housing_dataset.csv\")\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df[['City', 'Area']]\n",
    "y = df['Price']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess categorical columns using one-hot encoding\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('encoder', OneHotEncoder(drop='first'), [0])],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create a pipeline with preprocessing and random forest regressor\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('regressor', RandomForestRegressor(random_state=42))])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "pickle.dump(pipeline, open('model.pkl', 'wb'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-License-Plate-Detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
