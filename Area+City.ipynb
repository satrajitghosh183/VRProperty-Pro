{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Read individual CSV files and store them in a list\n",
    "file_paths = glob.glob(r\"D:\\Housing Prices Prediction\\All Metropolitan Cities\\*.csv\")\n",
    "dfs = []\n",
    "city_names = []\n",
    "for file_path in file_paths:\n",
    "    city_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    df = pd.read_csv(file_path)\n",
    "    dfs.append(df)\n",
    "    city_names.extend([city_name] * len(df))\n",
    "\n",
    "# Combine all dataframes and city names into one dataframe\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "combined_df[\"City\"] = city_names\n",
    "\n",
    "# Save the combined dataframe to a new CSV file\n",
    "combined_df.to_csv(\"housing_dataset.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Read individual CSV files and store them in a list\n",
    "file_paths = glob.glob(r\"D:\\Housing Prices Prediction\\All Metropolitan Cities\\*.csv\")\n",
    "dfs = []\n",
    "city_names = []\n",
    "for file_path in file_paths:\n",
    "    city_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    df = pd.read_csv(file_path)\n",
    "    dfs.append(df)\n",
    "    city_names.extend([city_name] * len(df))\n",
    "\n",
    "# Combine all dataframes and city names into one dataframe\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "combined_df[\"City\"] = city_names\n",
    "\n",
    "# Select 16,000 random data points\n",
    "random_df = combined_df.sample(n=16000, random_state=42)\n",
    "\n",
    "# Save the randomly selected dataframe to a new CSV file\n",
    "random_df.to_csv(\"random_housing_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Read the dataset\n",
    "df = pd.read_csv(\"housing_dataset.csv\")\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df[['City', 'Area']]\n",
    "y = df['Price']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess categorical columns using one-hot encoding\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('encoder', OneHotEncoder(drop='first'), [0])],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create a pipeline with preprocessing and random forest regressor\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('regressor', RandomForestRegressor(random_state=42))])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict house prices on the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model (you can use different evaluation metrics)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print('Mean Absolute Error:', mae)\n",
    "\n",
    "# Make a sample prediction\n",
    "sample_data = pd.DataFrame({'City': ['Bangalore'], 'Area': [1500]})\n",
    "predicted_price = pipeline.predict(sample_data)\n",
    "print('Predicted Price:', predicted_price[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('housing_dataset.csv')\n",
    "\n",
    "# Select the desired features and target variable\n",
    "features = ['Area', 'Location', 'No. of Bedrooms', 'Resale', 'MaintenanceStaff', 'Gymnasium',\n",
    "            'SwimmingPool', 'LandscapedGardens', 'JoggingTrack', 'IndoorGames', 'SportsFacility',\n",
    "            'CarParking', 'City']\n",
    "target = 'Price'\n",
    "\n",
    "# Prepare the data\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# Convert categorical variables 'Location' and 'City' into dummy/indicator variables\n",
    "X = pd.get_dummies(X, columns=['Location', 'City'], drop_first=True)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the RandomForestRegressor model\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate mean absolute error\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print('Mean Absolute Error:', mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from geopy.geocoders import Nominatim\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "# Step 1: Preprocess the data\n",
    "df = pd.read_csv('housing_dataset.csv')\n",
    "df = df.dropna()  # Dropping rows with missing values\n",
    "\n",
    "# Step 2: Geocode locations and calculate distances\n",
    "geolocator = Nominatim(user_agent=\"your_app_name\")\n",
    "\n",
    "def get_coordinates(location):\n",
    "    location_info = geolocator.geocode(location)\n",
    "    if location_info:\n",
    "        lat = location_info.latitude\n",
    "        lon = location_info.longitude\n",
    "        return lat, lon\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Get latitude and longitude for each location\n",
    "df[['Latitude', 'Longitude']] = df['Location'].apply(get_coordinates).apply(pd.Series)\n",
    "\n",
    "# Function to calculate distance using haversine formula\n",
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0  # Radius of the Earth in kilometers\n",
    "    lat1_rad = radians(lat1)\n",
    "    lon1_rad = radians(lon1)\n",
    "    lat2_rad = radians(lat2)\n",
    "    lon2_rad = radians(lon2)\n",
    "\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "\n",
    "    a = sin(dlat / 2) ** 2 + cos(lat1_rad) * cos(lat2_rad) * sin(dlon / 2) ** 2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "# Calculate distances between locations\n",
    "df['Distance'] = df.apply(\n",
    "    lambda row: calculate_distance(row['Latitude'], row['Longitude'], row['Latitude'], row['Longitude']), axis=1\n",
    ")\n",
    "\n",
    "# Step 3: Perform clustering based on city\n",
    "scaler = StandardScaler()\n",
    "df_scaled_city = scaler.fit_transform(df[['Latitude', 'Longitude']])\n",
    "kmeans_city = KMeans(n_clusters=3)  # Replace '3' with the desired number of clusters for cities\n",
    "city_clusters = kmeans_city.fit_predict(df_scaled_city)\n",
    "\n",
    "# Step 4: Perform clustering based on location within each city\n",
    "df['CityCluster'] = city_clusters  # Add city cluster labels to the dataframe\n",
    "df_clusters = []\n",
    "\n",
    "for city_cluster_id in range(len(kmeans_city.cluster_centers_)):\n",
    "    df_city_cluster = df[df['CityCluster'] == city_cluster_id]\n",
    "    df_scaled_location = scaler.fit_transform(df_city_cluster[['Distance']])\n",
    "    kmeans_location = KMeans(n_clusters=4)  # Replace '4' with the desired number of clusters for locations within each city\n",
    "    location_clusters = kmeans_location.fit_predict(df_scaled_location)\n",
    "    df_city_cluster['LocationCluster'] = location_clusters\n",
    "    df_clusters.append(df_city_cluster)\n",
    "\n",
    "# Step 5: Prepare the feature set for regression analysis\n",
    "features = [\n",
    "    'No. of Bedrooms', 'Resale', 'MaintenanceStaff', 'Gymnasium', 'SwimmingPool', 'LandscapedGardens',\n",
    "    'JoggingTrack', 'RainWaterHarvesting', 'IndoorGames', 'ShoppingMall', 'Intercom', 'SportsFacility',\n",
    "    'ATM', 'ClubHouse', 'School', '24X7Security', 'PowerBackup', 'CarParking'\n",
    "]\n",
    "\n",
    "# Remove 'Wifi' and 'Wardrobe' from features list\n",
    "features = [feature for feature in features if feature not in ['Wifi', 'Wardrobe']]\n",
    "\n",
    "# Step 6: Perform regression analysis for each cluster\n",
    "for df_cluster in df_clusters:\n",
    "    X = df_cluster[features]\n",
    "    y = df_cluster['Price']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    regression_model = LinearRegression()\n",
    "    regression_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = regression_model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    print(f\"Cluster Mean Squared Error: {mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from geopy.geocoders import Nominatim\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "import time\n",
    "\n",
    "# Step 1: Preprocess the data\n",
    "df = pd.read_csv('housing_dataset.csv')\n",
    "df = df.dropna()  # Dropping rows with missing values\n",
    "\n",
    "# Step 2: Geocode locations and calculate distances\n",
    "geolocator = Nominatim(user_agent=\"your_app_name\")\n",
    "\n",
    "def get_coordinates(location):\n",
    "    max_retries = 5  # Maximum number of retries\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            location_info = geolocator.geocode(location)\n",
    "            if location_info:\n",
    "                lat = location_info.latitude\n",
    "                lon = location_info.longitude\n",
    "                return lat, lon\n",
    "            else:\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Geocoding failed for location: {location}. Retrying...\")\n",
    "            retries += 1\n",
    "            time.sleep(1)  # Wait for 1 second before retrying\n",
    "    return None\n",
    "\n",
    "# Get latitude and longitude for each location\n",
    "df[['Latitude', 'Longitude']] = df['Location'].apply(get_coordinates).apply(pd.Series)\n",
    "\n",
    "# Function to calculate distance using haversine formula\n",
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0  # Radius of the Earth in kilometers\n",
    "    lat1_rad = radians(lat1)\n",
    "    lon1_rad = radians(lon1)\n",
    "    lat2_rad = radians(lat2)\n",
    "    lon2_rad = radians(lon2)\n",
    "\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "\n",
    "    a = sin(dlat / 2) ** 2 + cos(lat1_rad) * cos(lat2_rad) * sin(dlon / 2) ** 2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "# Calculate distances between locations\n",
    "df['Distance'] = df.apply(\n",
    "    lambda row: calculate_distance(row['Latitude'], row['Longitude'], row['Latitude'], row['Longitude']), axis=1\n",
    ")\n",
    "# Step 3: Perform clustering based on city\n",
    "scaler = StandardScaler()\n",
    "df_scaled_city = scaler.fit_transform(df[['Latitude', 'Longitude']])\n",
    "kmeans_city = KMeans(n_clusters=3)  # Replace '3' with the desired number of clusters for cities\n",
    "city_clusters = kmeans_city.fit_predict(df_scaled_city)\n",
    "\n",
    "# Step 4: Perform clustering based on location within each city\n",
    "df['CityCluster'] = city_clusters  # Add city cluster labels to the dataframe\n",
    "df_clusters = []\n",
    "\n",
    "for city_cluster_id in range(len(kmeans_city.cluster_centers_)):\n",
    "    df_city_cluster = df[df['CityCluster'] == city_cluster_id]\n",
    "    df_scaled_location = scaler.fit_transform(df_city_cluster[['Distance']])\n",
    "    kmeans_location = KMeans(n_clusters=4)  # Replace '4' with the desired number of clusters for locations within each city\n",
    "    location_clusters = kmeans_location.fit_predict(df_scaled_location)\n",
    "    df_city_cluster['LocationCluster'] = location_clusters\n",
    "    df_clusters.append(df_city_cluster)\n",
    "\n",
    "# Step 5: Prepare the feature set for regression analysis\n",
    "features = [\n",
    "    'No. of Bedrooms', 'Resale', 'MaintenanceStaff', 'Gymnasium', 'SwimmingPool', 'LandscapedGardens',\n",
    "    'JoggingTrack', 'RainWaterHarvesting', 'IndoorGames', 'ShoppingMall', 'Intercom', 'SportsFacility',\n",
    "    'ATM', 'ClubHouse', 'School', '24X7Security', 'PowerBackup', 'CarParking'\n",
    "]\n",
    "\n",
    "# Remove 'Wifi' and 'Wardrobe' from features list\n",
    "features = [feature for feature in features if feature not in ['Wifi', 'Wardrobe']]\n",
    "\n",
    "# Step 6: Perform regression analysis for each cluster\n",
    "for df_cluster in df_clusters:\n",
    "    X = df_cluster[features]\n",
    "    y = df_cluster['Price']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    regression_model = LinearRegression()\n",
    "    regression_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = regression_model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    print(f\"Cluster Mean Squared Error: {mse}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import urllib3\n",
    "import json\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "import time\n",
    "from urllib.parse import quote\n",
    "\n",
    "# Step 1: Preprocess the data\n",
    "df = pd.read_csv('housing_dataset.csv')\n",
    "df = df.dropna()  # Dropping rows with missing values\n",
    "\n",
    "# Step 2: Geocode locations and calculate distances\n",
    "http = urllib3.PoolManager(1, headers={'user-agent': 'my-test-app'})\n",
    "\n",
    "def get_coordinates(location):\n",
    "    max_retries = 1  # Maximum number of retries\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            encoded_location = quote(location)\n",
    "            url = f\"https://nominatim.openstreetmap.org/search?q={encoded_location}&format=json&limit=1\"\n",
    "            resp = http.request('GET', url)\n",
    "            if resp.status == 200:\n",
    "                location_info = json.loads(resp.data.decode())\n",
    "                if location_info:\n",
    "                    lat = float(location_info[0]['lat'])\n",
    "                    lon = float(location_info[0]['lon'])\n",
    "                    return lat, lon\n",
    "                else:\n",
    "                    return None\n",
    "            else:\n",
    "                print(f\"Geocoding failed for location: {location}. Status code: {resp.status}\")\n",
    "                retries += 1\n",
    "                time.sleep(1)  # Wait for 1 second before retrying\n",
    "        except Exception as e:\n",
    "            print(f\"Geocoding failed for location: {location}. Error: {e}\")\n",
    "            retries += 1\n",
    "            time.sleep(1)  # Wait for 1 second before retrying\n",
    "    return None\n",
    "\n",
    "# Get latitude and longitude for each location\n",
    "df[['Latitude', 'Longitude']] = df['Location'].apply(get_coordinates).apply(pd.Series)\n",
    "\n",
    "# Function to calculate distance using haversine formula\n",
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0  # Radius of the Earth in kilometers\n",
    "    lat1_rad = radians(lat1)\n",
    "    lon1_rad = radians(lon1)\n",
    "    lat2_rad = radians(lat2)\n",
    "    lon2_rad = radians(lon2)\n",
    "\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "\n",
    "    a = sin(dlat / 2) ** 2 + cos(lat1_rad) * cos(lat2_rad) * sin(dlon / 2) ** 2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "# Calculate distances between locations\n",
    "df['Distance'] = df.apply(\n",
    "    lambda row: calculate_distance(row['Latitude'], row['Longitude'], row['Latitude'], row['Longitude']), axis=1\n",
    ")\n",
    "\n",
    "# Step 3: Perform clustering based on city\n",
    "scaler = StandardScaler()\n",
    "df_scaled_city = scaler.fit_transform(df[['Latitude', 'Longitude']])\n",
    "kmeans_city = KMeans(n_clusters=3)  # Replace '3' with the desired number of clusters for cities\n",
    "city_clusters = kmeans_city.fit_predict(df_scaled_city)\n",
    "\n",
    "# Step 4: Perform clustering based on location within each city\n",
    "df['CityCluster'] = city_clusters  # Add city cluster labels to the dataframe\n",
    "df_clusters = []\n",
    "\n",
    "for city_cluster_id in range(len(kmeans_city.cluster_centers_)):\n",
    "    df_city_cluster = df[df['CityCluster'] == city_cluster_id]\n",
    "    df_scaled_location = scaler.fit_transform(df_city_cluster[['Distance']])\n",
    "    kmeans_location = KMeans(n_clusters=4)  # Replace '4' with the desired number of clusters for locations within each city\n",
    "    location_clusters = kmeans_location.fit_predict(df_scaled_location)\n",
    "    df_city_cluster['LocationCluster'] = location_clusters\n",
    "    df_clusters.append(df_city_cluster)\n",
    "\n",
    "# Step 5: Prepare the feature set for regression analysis\n",
    "features = [\n",
    "    'No. of Bedrooms', 'Resale', 'MaintenanceStaff', 'Gymnasium', 'SwimmingPool', 'LandscapedGardens',\n",
    "    'JoggingTrack', 'RainWaterHarvesting', 'IndoorGames', 'ShoppingMall', 'Intercom', 'SportsFacility',\n",
    "    'ATM', 'ClubHouse', 'School', '24X7Security', 'PowerBackup', 'CarParking'\n",
    "]\n",
    "\n",
    "# Remove 'Wifi' and 'Wardrobe' from features list\n",
    "features = [feature for feature in features if feature not in ['Wifi', 'Wardrobe']]\n",
    "\n",
    "# Step 6: Perform regression analysis for each cluster\n",
    "for df_cluster in df_clusters:\n",
    "    X = df_cluster[features]\n",
    "    y = df_cluster['Price']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    regression_model = LinearRegression()\n",
    "    regression_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = regression_model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    print(f\"Cluster Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import urllib3\n",
    "import json\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "import time\n",
    "from urllib.parse import quote\n",
    "\n",
    "# Step 1: Preprocess the data\n",
    "df = pd.read_csv('housing_dataset.csv')\n",
    "df = df.dropna()  # Dropping rows with missing values\n",
    "\n",
    "# Step 2: Geocode locations and calculate distances\n",
    "http = urllib3.PoolManager(1, headers={'user-agent': 'my-test-app'})\n",
    "\n",
    "def get_coordinates(location):\n",
    "    max_retries = 3  # Maximum number of retries\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            encoded_location = quote(location)\n",
    "            url = f\"https://nominatim.openstreetmap.org/search?q={encoded_location}&format=json&limit=1\"\n",
    "            print(f\"Geocoding request for location: {location}\")\n",
    "            resp = http.request('GET', url, timeout=5.0)  # Set timeout value to 5 seconds\n",
    "            if resp.status == 200:\n",
    "                location_info = json.loads(resp.data.decode())\n",
    "                if location_info:\n",
    "                    lat = float(location_info[0]['lat'])\n",
    "                    lon = float(location_info[0]['lon'])\n",
    "                    print(f\"Geocoding successful for location: {location}. Latitude: {lat}, Longitude: {lon}\")\n",
    "                    return lat, lon\n",
    "                else:\n",
    "                    print(f\"No geocoding results found for location: {location}\")\n",
    "                    return None\n",
    "            else:\n",
    "                print(f\"Geocoding failed for location: {location}. Status code: {resp.status}\")\n",
    "                retries += 1\n",
    "                time.sleep(1)  # Wait for 1 second before retrying\n",
    "        except Exception as e:\n",
    "            print(f\"Geocoding failed for location: {location}. Error: {e}\")\n",
    "            retries += 1\n",
    "            time.sleep(1)  # Wait for 1 second before retrying\n",
    "    return None\n",
    "\n",
    "# Get latitude and longitude for each location\n",
    "df[['Latitude', 'Longitude']] = df['Location'].apply(get_coordinates).apply(pd.Series)\n",
    "\n",
    "# Function to calculate distance using haversine formula\n",
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0  # Radius of the Earth in kilometers\n",
    "    lat1_rad = radians(lat1)\n",
    "    lon1_rad = radians(lon1)\n",
    "    lat2_rad = radians(lat2)\n",
    "    lon2_rad = radians(lon2)\n",
    "\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "\n",
    "    a = sin(dlat / 2) ** 2 + cos(lat1_rad) * cos(lat2_rad) * sin(dlon / 2) ** 2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "# Calculate distances between locations\n",
    "df['Distance'] = df.apply(\n",
    "    lambda row: calculate_distance(row['Latitude'], row['Longitude'], row['Latitude'], row['Longitude']), axis=1\n",
    ")\n",
    "\n",
    "# Rest of the code...\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "\n",
    "# Step 3: Perform clustering based on city\n",
    "scaler = StandardScaler()\n",
    "df_scaled_city = scaler.fit_transform(df[['Latitude', 'Longitude']])\n",
    "kmeans_city = KMeans(n_clusters=6)  # Replace '3' with the desired number of clusters for cities\n",
    "city_clusters = kmeans_city.fit_predict(df_scaled_city)\n",
    "\n",
    "# Step 4: Perform clustering based on location within each city\n",
    "df['CityCluster'] = city_clusters  # Add city cluster labels to the dataframe\n",
    "df_clusters = []\n",
    "\n",
    "for city_cluster_id in range(len(kmeans_city.cluster_centers_)):\n",
    "    df_city_cluster = df[df['CityCluster'] == city_cluster_id]\n",
    "    df_scaled_location = scaler.fit_transform(df_city_cluster[['Distance']])\n",
    "    kmeans_location = KMeans(n_clusters=50)  # Replace '4' with the desired number of clusters for locations within each city\n",
    "    location_clusters = kmeans_location.fit_predict(df_scaled_location)\n",
    "    df_city_cluster['LocationCluster'] = location_clusters\n",
    "    df_clusters.append(df_city_cluster)\n",
    "\n",
    "# Step 5: Prepare the feature set for regression analysis\n",
    "features = [\n",
    "    'No. of Bedrooms', 'Resale', 'MaintenanceStaff', 'Gymnasium', 'SwimmingPool', 'LandscapedGardens',\n",
    "    'JoggingTrack', 'RainWaterHarvesting', 'IndoorGames', 'ShoppingMall', 'Intercom', 'SportsFacility',\n",
    "    'ATM', 'ClubHouse', 'School', '24X7Security', 'PowerBackup', 'CarParking'\n",
    "]\n",
    "\n",
    "# Remove 'Wifi' and 'Wardrobe' from features list\n",
    "features = [feature for feature in features if feature not in ['Wifi', 'Wardrobe']]\n",
    "\n",
    "# Step 6: Perform regression analysis for each cluster\n",
    "for df_cluster in df_clusters:\n",
    "    X = df_cluster[features]\n",
    "    y = df_cluster['Price']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    regression_model = LinearRegression()\n",
    "    regression_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = regression_model.predict(X_test)\n",
    "    print(y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    print(f\"Cluster Mean Squared Error: {mse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-License-Plate-Detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
